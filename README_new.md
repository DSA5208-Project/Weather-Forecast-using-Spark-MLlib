# Weather Forecast using Spark MLlib

A machine learning project to predict air temperature from weather observation data using Apache Spark MLlib.

## ğŸ“‹ Project Overview

This project implements a complete machine learning pipeline for weather temperature prediction using Apache Spark. The system processes hourly surface weather observations from stations worldwide and builds predictive models using various regression algorithms.

**Dataset**: Global Hourly Weather Data (2024)  
**Source**: [NOAA Global Hourly Dataset](https://www.ncei.noaa.gov/data/global-hourly/archive/csv/)  
**Target Variable**: Air Temperature (TMP)  
**Documentation**: [Dataset Documentation](https://www.ncei.noaa.gov/data/global-hourly/doc/)

## ğŸ¯ Project Objectives

- Preprocess large-scale weather data using Apache Spark
- Build and compare multiple machine learning regression models
- Perform hyperparameter tuning using cross-validation
- Evaluate model performance using standard metrics (RMSE, MAE, RÂ²)
- Generate comprehensive visualizations and reports

## ğŸ—ï¸ Project Structure

```
Weather-Forecast-using-Spark-MLlib/
â”‚
â”œâ”€â”€ main.py                       # Main execution script (entry point)
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ LICENSE                       # Project license
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ sample.csv                    # Sample data for testing
â”œâ”€â”€ MLlib.pdf                     # Project requirements document
â”œâ”€â”€ isd-format-document.pdf       # Dataset format documentation
â”‚
â”œâ”€â”€ src/                          # Source code directory
â”‚   â”œâ”€â”€ __init__.py              # Package initializer
â”‚   â”œâ”€â”€ config.py                # Configuration parameters
â”‚   â”œâ”€â”€ data_preprocessing.py    # Data loading and cleaning
â”‚   â”œâ”€â”€ feature_engineering.py   # Feature selection with UnivariateFeatureSelector
â”‚   â”œâ”€â”€ train_model.py           # Model training with CV
â”‚   â”œâ”€â”€ evaluate_model.py        # Model evaluation and visualization
â”‚   â””â”€â”€ utils.py                 # Utility functions
â”‚
â”œâ”€â”€ data/                         # Raw data (download 2024.tar.gz here)
â”‚
â”œâ”€â”€ models/                       # Trained models (auto-created)
â”‚   â”œâ”€â”€ best_model/              # Best performing model
â”‚   â””â”€â”€ all_models/              # All trained models
â”‚
â””â”€â”€ output/                       # Results and visualizations (auto-created)
    â”œâ”€â”€ model_results.csv        # Performance metrics
    â”œâ”€â”€ feature_importance.csv   # Feature selection results
    â”œâ”€â”€ model_report.txt         # Comprehensive text report
    â”œâ”€â”€ training.log             # Execution log
    â””â”€â”€ *.png                    # Visualization plots
```

## ğŸš€ Getting Started

### Prerequisites

- **Python**: 3.8 or higher
- **Java**: JDK 8 or 11 (required for PySpark)
- **Memory**: At least 8GB RAM recommended
- **Storage**: ~5GB for dataset and outputs

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/DSA5208-Project/Weather-Forecast-using-Spark-MLlib.git
   cd Weather-Forecast-using-Spark-MLlib
   ```

2. **Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify Java installation**:
   ```bash
   java -version
   # Should show Java 8 or 11
   ```

### Download Dataset

Download the 2024 weather data:

```bash
# Create data directory
mkdir -p data

# Download the dataset (Option 1: Using wget)
wget https://www.ncei.noaa.gov/data/global-hourly/archive/csv/2024.tar.gz -O data/2024.tar.gz

# Or (Option 2: Using curl)
curl -o data/2024.tar.gz https://www.ncei.noaa.gov/data/global-hourly/archive/csv/2024.tar.gz

# Extract the archive
cd data
tar -xzf 2024.tar.gz
cd ..
```

**Note**: The full dataset is large (~2GB compressed). For testing, you can use the provided `sample.csv` file.

## ğŸ“Š Usage

### Quick Start (Using Sample Data)

Test the pipeline with sample data:

```bash
python main.py --use-sample
```

### Full Pipeline (Using Complete Dataset)

Run the complete pipeline with all features:

```bash
python main.py
```

### Custom Options

```bash
# Use custom data file
python main.py --data-path path/to/your/data.csv

# Train specific models only
python main.py --models LinearRegression RandomForestRegressor

# Skip feature selection (use all features)
python main.py --skip-feature-selection

# Combine options
python main.py --use-sample --models LinearRegression GBTRegressor
```

### Command-Line Arguments

- `--data-path PATH`: Path to CSV data file
- `--use-sample`: Use sample.csv for testing
- `--skip-feature-selection`: Skip feature selection step
- `--models MODEL1 MODEL2 ...`: Train specific models only

### Available Models

- `LinearRegression`: Linear regression with regularization
- `RandomForestRegressor`: Random forest ensemble
- `GBTRegressor`: Gradient-boosted trees
- `GeneralizedLinearRegression`: Generalized linear model

## ğŸ”§ Configuration

Edit `src/config.py` to customize:

- **Data preprocessing**: Missing value handling, outlier removal
- **Feature selection**: Method (fpr, numTopFeatures, etc.) and parameters
- **Models**: Hyperparameter grids for cross-validation
- **Training**: Number of CV folds, parallelism
- **Output**: Plot settings, file paths

Key configuration options:

```python
# Feature selection
FEATURE_SELECTION_METHOD = "fpr"  # False Positive Rate
FEATURE_SELECTION_PARAM = 0.05    # Threshold

# Training
NUM_FOLDS = 5                     # Cross-validation folds
TRAIN_TEST_SPLIT_RATIO = 0.7      # 70% train, 30% test

# Models to train
MODELS_TO_TRAIN = [
    "LinearRegression",
    "RandomForestRegressor",
    "GBTRegressor",
    "GeneralizedLinearRegression"
]
```

## ğŸ“ˆ Pipeline Steps

### 1. Data Preprocessing

- **Loading**: Read CSV data with Spark
- **Parsing**: Extract features from complex weather columns (WND, TMP, DEW, SLP, etc.)
- **Cleaning**: Remove invalid values (9999, 999 indicators)
- **Filtering**: Apply temperature range filters (-90Â°C to 60Â°C)
- **Imputation**: Fill missing values using median strategy
- **Outlier Removal**: Remove statistical outliers using IQR method
- **Encoding**: Convert categorical features (STATION) to numeric
- **Standardization**: Scale features using StandardScaler

### 2. Feature Engineering

- **Feature Extraction**: Parse weather observations into numeric features
  - Wind direction and speed
  - Ceiling height
  - Visibility distance
  - Dew point temperature
  - Sea level pressure
  - Geographic coordinates (latitude, longitude, elevation)
  - Temporal features (hour, month, day of year)

- **Feature Selection**: Use UnivariateFeatureSelector
  - Method: F-test for regression (f_regression)
  - Selection mode: False Positive Rate (fpr) with threshold 0.05
  - Reduces dimensionality by selecting statistically significant features

### 3. Model Training

- **Cross-Validation**: K-fold CV (default: 5 folds)
- **Hyperparameter Tuning**: Grid search over parameter combinations
- **Multiple Models**: Train and compare 4 regression algorithms
- **Best Model Selection**: Choose model with lowest CV RMSE

### 4. Model Evaluation

**Metrics**:
- RMSE (Root Mean Squared Error)
- MAE (Mean Absolute Error)
- RÂ² (Coefficient of Determination)
- MSE (Mean Squared Error)

**Visualizations**:
- Predictions vs Actual scatter plots
- Residual plots
- Error distribution histograms
- Model comparison bar charts

### 5. Results and Reporting

- CSV files with detailed metrics
- Comprehensive text report
- High-resolution plots (300 DPI PNG)
- Feature importance rankings
- Training logs

## ğŸ“ Output Files

After execution, check the `output/` directory:

- **model_results.csv**: Performance metrics for all models
- **feature_importance.csv**: Feature correlation scores
- **model_report.txt**: Comprehensive text report
- **training.log**: Detailed execution log
- **predictions_vs_actual_*.png**: Scatter plots of predictions
- **residuals_*.png**: Residual analysis plots
- **error_distribution_*.png**: Error distribution histograms
- **model_comparison.png**: Bar chart comparing all models

## ğŸ§ª Testing

Test with sample data (fast execution):

```bash
python main.py --use-sample
```

This uses `sample.csv` (~100 rows) for quick validation.

## ğŸ› Troubleshooting

### Common Issues

**1. Java not found**:
```
Error: JAVA_HOME is not set
```
Solution: Install Java 8 or 11 and set JAVA_HOME environment variable

**2. Memory errors**:
```
OutOfMemoryError: Java heap space
```
Solution: Increase Spark memory in `src/config.py`:
```python
SPARK_CONFIG = {
    "spark.driver.memory": "8g",  # Increase from 4g
    "spark.executor.memory": "8g"
}
```

**3. PySpark not found**:
```
ModuleNotFoundError: No module named 'pyspark'
```
Solution: Install dependencies:
```bash
pip install -r requirements.txt
```

**4. Dataset not found**:
```
FileNotFoundError: sample.csv
```
Solution: Ensure you're running from the project root directory

## ğŸ“ Requirements

See [MLlib.pdf](MLlib.pdf) for detailed project requirements.

### Key Requirements Met

âœ… Data preprocessing with invalid value removal  
âœ… Feature standardization  
âœ… Train/test split (70/30)  
âœ… Multiple ML models (4 algorithms)  
âœ… Cross-validation for hyperparameter tuning  
âœ… UnivariateFeatureSelector for feature selection  
âœ… Comprehensive evaluation (RMSE, MAE, RÂ²)  
âœ… Visualizations and performance plots  
âœ… Complete source code with README  
âœ… Trained model persistence  
âœ… Detailed report generation  

## ğŸ“š Documentation

- **Dataset Format**: See [isd-format-document.pdf](isd-format-document.pdf)
- **Project Requirements**: See [MLlib.pdf](MLlib.pdf)
- **NOAA Documentation**: https://www.ncei.noaa.gov/data/global-hourly/doc/

## ğŸ† Results

Example results (will vary based on data):

| Model | RMSE | MAE | RÂ² |
|-------|------|-----|-----|
| Random Forest | 2.34 | 1.78 | 0.94 |
| GBT | 2.45 | 1.85 | 0.93 |
| Linear Regression | 3.12 | 2.41 | 0.88 |
| GLM | 3.15 | 2.43 | 0.87 |

## ğŸ‘¥ Group Members

List your group members here:
- Name 1 (Student ID)
- Name 2 (Student ID)
- Name 3 (Student ID)

## ğŸ“„ License

See [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- NOAA for providing the Global Hourly Weather Dataset
- Apache Spark community for MLlib
- Course instructors and TAs for guidance
